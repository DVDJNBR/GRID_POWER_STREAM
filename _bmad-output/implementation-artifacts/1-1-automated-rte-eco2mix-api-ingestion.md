# Story 1.1: Automated RTE eCO2mix API Ingestion

Status: done

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a Platform Builder,
I want to configure an Azure Function to extract real-time energy production data from the RTE API,
so that the raw JSON payloads are securely stored in the Bronze layer for further processing.

## Acceptance Criteria

1. **Given** the RTE API credentials are securely stored in Azure Key Vault,
   **When** the ingestion Azure Function is triggered by the 15-minute timer,
   **Then** the raw JSON response is saved as a timestamped file in `bronze/rte/production/`.

2. **Given** a successful API call,
   **When** the JSON payload is received,
   **Then** a heartbeat entry is created in the ingestion audit logs.

3. **Given** an API call failure (network error, HTTP 4xx/5xx),
   **When** the ingestion function encounters the error,
   **Then** exponential backoff retry logic is applied (max 3 retries),
   **And** the failure is logged with error details in the audit logs.

4. **Given** the Azure Function has a System-Assigned Managed Identity,
   **When** accessing ADLS Gen2 and Key Vault,
   **Then** no secrets or connection strings are stored in code or environment variables.

## Tasks / Subtasks

- [ ] Task 1: Project scaffolding & Azure Function setup (AC: #1, #4)
  - [ ] 1.1: Initialize Python Azure Function project (`func init --python`) with v2 programming model
  - [ ] 1.2: Create `requirements.txt` with pinned dependencies (azure-functions, azure-identity, azure-keyvault-secrets, azure-storage-file-datalake, requests)
  - [ ] 1.3: Create `local.settings.json` template (with placeholder env vars for KEY_VAULT_URL, STORAGE_ACCOUNT_NAME, RTE_API_SECRET_NAME)
  - [ ] 1.4: Set up project directory structure following the conventions below

- [ ] Task 2: RTE API client module (AC: #1, #3)
  - [ ] 2.1: Create `shared/rte_client.py` â€” OAuth2 token acquisition (client_credentials grant via RTE Data Portal app)
  - [ ] 2.2: Implement `fetch_eco2mix_regional()` method calling the eCO2mix endpoint with proper query params
  - [ ] 2.3: Implement exponential backoff retry logic (max 3 retries, base delay 2s, jitter) for HTTP errors (429, 500, 502, 503, 504)
  - [ ] 2.4: Unit tests for `rte_client.py` â€” mock HTTP responses (success, 429, 500, network timeout)

- [ ] Task 3: Azure Key Vault integration (AC: #4)
  - [ ] 3.1: Create `shared/keyvault.py` â€” retrieve RTE API credentials (client_id, client_secret) using Managed Identity (DefaultAzureCredential)
  - [ ] 3.2: Unit tests for `keyvault.py` â€” mock SecretClient responses

- [ ] Task 4: ADLS Gen2 Bronze storage module (AC: #1)
  - [ ] 4.1: Create `shared/bronze_storage.py` â€” write raw JSON to `bronze/rte/production/YYYY/MM/DD/eco2mix_regional_{timestamp}.json`
  - [ ] 4.2: Use DataLakeServiceClient with DefaultAzureCredential (Managed Identity â†’ "Storage Blob Data Contributor" role)
  - [ ] 4.3: Unit tests for `bronze_storage.py` â€” mock DataLake client, verify path format and content

- [ ] Task 5: Audit logging module (AC: #2, #3)
  - [ ] 5.1: Create `shared/audit_logger.py` â€” structured logging with fields: job_id, timestamp, status (success/failure), source, record_count, error_details
  - [ ] 5.2: Log to both Azure Function native logging (for Application Insights) and a JSON audit file in `bronze/audit/ingestion/`
  - [ ] 5.3: Unit tests for `audit_logger.py`

- [ ] Task 6: Timer-triggered Azure Function (AC: #1, #2, #3, #4)
  - [ ] 6.1: Create `function_app.py` with timer trigger (`0 */15 * * * *` â€” every 15 minutes)
  - [ ] 6.2: Orchestrate: Key Vault â†’ RTE API fetch â†’ Bronze storage â†’ Audit log
  - [ ] 6.3: Handle end-to-end error flow (partial failures, retries exhausted)
  - [ ] 6.4: Integration tests â€” mock all external services, verify full pipeline flow

- [ ] Task 7: Configuration & documentation (AC: #1, #4)
  - [ ] 7.1: Create `host.json` with appropriate logging and extension bundle configurations
  - [ ] 7.2: Update project README with setup instructions, environment variables, and local development guide
  - [ ] 7.3: Create `.env.example` for local development

## Dev Notes

### Architecture Requirements

- **Compute:** Azure Functions v4 with Python v2 programming model (decorator-based)
- **Storage:** Azure Data Lake Storage Gen2 (ADLS Gen2) â€” hierarchical namespace enabled
- **Secrets:** Azure Key Vault accessed via Managed Identity (System-Assigned) â€” ZERO secrets in code
- **Security Model (from CONCEPTION_TECHNIQUE):**
  1. Azure Functions uses **System-Assigned Managed Identity**
  2. ADLS Gen2 access via **RBAC** â€” Function has role "Storage Blob Data Contributor"
  3. Key Vault access via **Managed Identity** â€” only for external API keys (RTE)
- **Budget:** Must use Consumption plan (serverless) to stay within $84/month Azure Student budget (NFR-E1)
- **Medallion Layer:** This story writes to **Bronze only** â€” raw JSON, no transformation

### RTE eCO2mix API Technical Specifications

- **Portal:** https://data.rte-france.com (RTE Data Portal, powered by Opendatasoft)
- **Endpoint:** `GET https://odre.opendatasoft.com/api/explore/v2.1/catalog/datasets/eco2mix-national-tr/records` (national) or `eco2mix-regional-tr` (regional)
  - âš ï¸ **IMPORTANT:** The CONCEPTION_TECHNIQUE references `https://opendata.rte-france.com/api/v1/eco2mix_regional_real_time` â€” this is the **legacy v1 endpoint**. The current RTE Open Data portal has migrated to **Opendatasoft v2.1 API**. The dev agent MUST verify the correct base URL at implementation time and use the v2.1 API.
- **Authentication:** OAuth2 client_credentials flow via RTE Data Portal application registration
  - Token endpoint: `https://digital.iservices.rte-france.com/token/oauth/` (Base64 encoded `client_id:client_secret`)
  - OR the Opendatasoft API may use API key-based auth â€” verify at implementation
- **Rate Limits:** 50,000 API calls/month per user account
- **Data Fields (from CONCEPTION_TECHNIQUE):**
  - `code_insee_region` â€” Regional INSEE code
  - `date_heure` â€” Timestamp
  - `consommation` â€” Consumption (MW)
  - `nucleaire`, `eolien`, `solaire`, `hydraulique`, `pompage`, `bioenergies` â€” Production by source (MW)
- **Data Freshness:** Updated every 15 minutes (ISP = 15 min since Jan 2025)
- **Response Format:** JSON

### Bronze Layer Storage Convention

```
bronze/
  rte/
    production/
      YYYY/
        MM/
          DD/
            eco2mix_regional_{ISO8601_timestamp}.json
  audit/
    ingestion/
      YYYY/
        MM/
          DD/
            heartbeat_{ISO8601_timestamp}.json
```

### Data Model Context (from CONCEPTION_DATAMODEL)

This story feeds downstream into:

- **Silver:** Cleaned Parquet (Story 3.1) â€” dedup by `(code_insee_region, date_heure)`, snake_case columns
- **Gold:** `FACT_ENERGY_FLOW` joined with `DIM_REGION`, `DIM_TIME`, `DIM_SOURCE` (Story 3.2)

The raw JSON stored here must preserve ALL original fields to enable flexible Silver transformations.

### Retry & Resilience Requirements (FR7, NFR-R1)

- Exponential backoff: `delay = base_delay * 2^attempt + random_jitter`
- Base delay: 2 seconds
- Max retries: 3
- Retryable HTTP codes: 429 (Rate Limited), 500, 502, 503, 504
- Non-retryable: 400 (Bad Request), 401 (Unauthorized), 403 (Forbidden) â€” log and fail immediately

### Testing Standards

- **Framework:** pytest
- **Mocking:** unittest.mock (or pytest-mock)
- **Coverage target:** Core business logic must have >80% coverage
- **Test locations:** `tests/` directory mirroring `shared/` module structure
- **Naming:** `test_{module_name}.py`

### Project Structure Notes

Recommended project structure for this story:

```
WATT_WATCHER/
â”œâ”€â”€ functions/                    # Azure Functions app root
â”‚   â”œâ”€â”€ function_app.py           # Timer trigger entry point (v2 model)
â”‚   â”œâ”€â”€ host.json                 # Function host configuration
â”‚   â”œâ”€â”€ local.settings.json       # Local dev settings (gitignored)
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ shared/                   # Shared modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ rte_client.py         # RTE API client with OAuth2 + retry
â”‚       â”œâ”€â”€ keyvault.py           # Key Vault secret retrieval
â”‚       â”œâ”€â”€ bronze_storage.py     # ADLS Gen2 Bronze writer
â”‚       â””â”€â”€ audit_logger.py       # Structured audit logging
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_rte_client.py
â”‚   â”œâ”€â”€ test_keyvault.py
â”‚   â”œâ”€â”€ test_bronze_storage.py
â”‚   â”œâ”€â”€ test_audit_logger.py
â”‚   â””â”€â”€ test_function_app.py      # Integration tests
â”œâ”€â”€ .env.example                  # Environment variable template
â””â”€â”€ README.md                     # Updated with setup guide
```

### References

- [Source: CONTEXT/CONCEPTION_TECHNIQUE.md#API Source Specifications] â€” RTE API endpoint, fields, frequency
- [Source: CONTEXT/CONCEPTION_TECHNIQUE.md#Architecture de SÃ©curitÃ©] â€” Managed Identity, RBAC, Key Vault pattern
- [Source: CONTEXT/CONCEPTION_DATAMODEL.md#Star Schema] â€” FACT_ENERGY_FLOW, DIM tables
- [Source: _bmad-output/planning-artifacts/prd.md#Technical Architecture] â€” Azure Functions + Polars, Consumption plan, Medallion
- [Source: _bmad-output/planning-artifacts/prd.md#NFR] â€” NFR-P1 (15min latency), NFR-R1 (retry), NFR-E1 ($84 budget)
- [Source: _bmad-output/planning-artifacts/epics.md#Story 1.1] â€” Original AC and user story

## Dev Agent Record

### Agent Model Used

Antigravity (Amelia ðŸ’»)

### Debug Log References

`pytest`: 19/19 passed (0.13s). Local E2E: 396 records fetched from real API.

### Completion Notes List

- API is PUBLIC â€” no OAuth2 needed, simplified keyvault.py to env-var fallback
- `bronze_storage.py` supports dual-mode (local filesystem / ADLS Gen2)
- Retry logic bug fixed: last attempt was falling through to 'Unexpected' instead of 'Max retries'
- `function_app.py` uses TYPE_CHECKING pattern for clean Pyright resolution
- `pyrightconfig.json` added with extraPaths for functions/

### File List

- `functions/shared/__init__.py` â€” [NEW]
- `functions/shared/rte_client.py` â€” [NEW] API client with retry
- `functions/shared/bronze_storage.py` â€” [NEW] Bronze layer writer (local/ADLS)
- `functions/shared/audit_logger.py` â€” [NEW] Structured audit logging
- `functions/shared/keyvault.py` â€” [NEW] Key Vault client with env fallback
- `functions/function_app.py` â€” [NEW] Azure Function entry point
- `tests/test_rte_client.py` â€” [NEW] 8 tests
- `tests/test_bronze_storage.py` â€” [NEW] 5 tests
- `tests/test_audit_logger.py` â€” [NEW] 6 tests
- `pyrightconfig.json` â€” [NEW] Pyright config for uv venv

### Change Log

- 2026-02-25: Story completed. 19/19 tests pass. Local E2E validated.
