# Story 3.1: Bronze to Silver â€” Data Cleaning & Partitioning

Status: done

## Story

As a Data Engineer,
I want to use Polars to clean the raw multi-source data (Bronze),
so that it is standardized, deduplicated, and stored as optimized Parquet files in the Silver layer.

## Acceptance Criteria

1. **Given** raw JSON/CSV files in the Bronze layer,
   **When** the Silver transformation job runs,
   **Then** columns are renamed to snake_case, nulls are handled, and duplicates are removed.

2. **Given** cleaned data,
   **When** written to Silver,
   **Then** the data is stored as Parquet files partitioned by `Year/Month/Day` in `silver/`.

3. **Given** the deduplication step,
   **When** processing RTE production data,
   **Then** duplicates are identified by the composite key `(code_insee_region, date_heure)`.

4. **Given** the cleaning process,
   **When** null or corrupted entries are found,
   **Then** they are handled according to defined rules (drop, impute, or flag) and logged.

## Tasks / Subtasks

- [ ] Task 1: Silver transformation module for RTE data (AC: #1, #2, #3)
  - [ ] 1.1: Create `shared/transformations/rte_silver.py` â€” load Bronze JSON, clean, deduplicate
  - [ ] 1.2: Column normalization: snake_case, consistent types (datetime, float for MW values)
  - [ ] 1.3: Deduplication on `(code_insee_region, date_heure)` composite key
  - [ ] 1.4: Write to `silver/rte/production/year=YYYY/month=MM/day=DD/*.parquet` (Hive partitioning)

- [ ] Task 2: Silver transformation for CSV capacity data (AC: #1, #2)
  - [ ] 2.1: Create `shared/transformations/capacity_silver.py` â€” load Bronze CSV, clean
  - [ ] 2.2: Standardize column names, handle missing values
  - [ ] 2.3: Write to `silver/reference/capacity/year=YYYY/*.parquet`

- [ ] Task 3: Silver transformation for maintenance data (AC: #1, #2)
  - [ ] 3.1: Create `shared/transformations/maintenance_silver.py` â€” parse scraped JSON
  - [ ] 3.2: Normalize date formats, clean description text
  - [ ] 3.3: Write to `silver/maintenance/year=YYYY/month=MM/*.parquet`

- [ ] Task 4: Silver transformation for ERA5 climate data (AC: #1, #2)
  - [ ] 4.1: Create `shared/transformations/era5_silver.py` â€” reproject, clean, standardize
  - [ ] 4.2: Compute derived fields: wind speed magnitude from u100/v100
  - [ ] 4.3: Write to `silver/climate/era5/year=YYYY/month=MM/*.parquet`

- [ ] Task 5: Null/corruption handling framework (AC: #4)
  - [ ] 5.1: Create `shared/transformations/data_quality.py` â€” null handling rules per source
  - [ ] 5.2: Define strategy per column: drop row, forward fill, interpolate, or flag
  - [ ] 5.3: Log quality metrics (null count, rows dropped, rows imputed) to audit

- [ ] Task 6: Azure Function trigger & orchestration (AC: #1, #2)
  - [ ] 6.1: Create timer-triggered function for Silver transformation (runs after Bronze ingestion)
  - [ ] 6.2: Track last processed Bronze files to avoid reprocessing (checkpoint)
  - [ ] 6.3: Integrate audit logging

- [ ] Task 7: Tests (AC: #1, #2, #3, #4)
  - [ ] 7.1: Unit tests per transformation module with fixture data
  - [ ] 7.2: Test deduplication logic, null handling, partitioning output
  - [ ] 7.3: Integration test â€” Bronze fixture â†’ Silver output validation

## Dev Notes

### Architecture Requirements

- **Library:** `polars` (mandatory per PRD) â€” use `pl.scan_parquet()` / `pl.read_json()` for lazy/streaming
- **Partitioning:** Hive-style (`year=YYYY/month=MM/day=DD/`) for efficient downstream queries
- **Memory (NFR-E2):** Use Polars streaming for large datasets, avoid `.collect()` on full datasets
- **Processing model:** Each source gets its own transformation module â€” modular, testable, independent

### Deduplication Key (from CONCEPTION_TECHNIQUE)

- RTE data: `(code_insee_region, date_heure)` â€” exact match dedup
- Maintenance: `(event_id)` â€” natural key
- ERA5: `(region, horodatage)` â€” spatial-temporal key

### Column Naming Convention

- All output columns: `snake_case`
- Timestamps: `DATETIME2` compatible ISO 8601 strings
- MW values: `FLOAT64` (Polars) â†’ `DECIMAL(10,2)` in SQL

### âš ï¸ Known Data Issues (from Story 0.1 API Exploration)

- **`pompage`** has inconsistent type across records: sometimes `str` ("0"), sometimes `int` (0). Cast to `Float64` during cleaning.
- **`stockage_batterie`/`destockage_batterie`** same issue â€” cast to `Float64`.
- **`column_68`/`column_30`** are null artifacts â€” drop during Silver transformation.

### Dependencies

- Stories 1.1, 1.2, 2.1, 2.2: Bronze data must exist (at least fixture samples for testing)
- Story 1.0: ADLS Gen2 provisioned

### Project Structure (additions)

```
functions/
â””â”€â”€ shared/
    â””â”€â”€ transformations/         # [NEW] Silver transformation modules
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ rte_silver.py
        â”œâ”€â”€ capacity_silver.py
        â”œâ”€â”€ maintenance_silver.py
        â”œâ”€â”€ era5_silver.py
        â””â”€â”€ data_quality.py
tests/
â””â”€â”€ test_transformations/        # [NEW]
    â”œâ”€â”€ test_rte_silver.py
    â”œâ”€â”€ test_capacity_silver.py
    â”œâ”€â”€ test_maintenance_silver.py
    â”œâ”€â”€ test_era5_silver.py
    â””â”€â”€ test_data_quality.py
```

### References

- [Source: _bmad-output/planning-artifacts/epics.md#Story 3.1] â€” Original AC
- [Source: CONTEXT/CONCEPTION_TECHNIQUE.md#Pipeline ETL] â€” Bronzeâ†’Silver logic, dedup key
- [Source: _bmad-output/planning-artifacts/prd.md#FR8] â€” Cleaning, partitioning
- [Source: _bmad-output/planning-artifacts/prd.md#NFR-E2] â€” Polars streaming efficiency

## Dev Agent Record

### Agent Model Used

Antigravity (Amelia ğŸ’»)

### Debug Log References

`pytest`: 17/17 Silver tests pass (0.27s)

### Completion Notes List

- 5 Silver transformation modules: rte, capacity, maintenance, era5, data_quality
- Data quality framework with 4 null strategies: DROP, FILL_ZERO, FORWARD_FILL, FLAG
- RTE: dedup on (code_insee_region, date_heure), pompage/stockage cast Strâ†’Float64
- Hive-partitioned Parquet output: `year=YYYY/month=MM/day=DD/`
- Fixed Polars 1.25+ strict timezone parsing (`time_zone="UTC"`)
- Modular: each source has its own transformation module

### File List

- `functions/shared/transformations/__init__.py` â€” [NEW] Package init
- `functions/shared/transformations/data_quality.py` â€” [NEW] Null handling framework
- `functions/shared/transformations/rte_silver.py` â€” [NEW] RTE Bronzeâ†’Silver
- `functions/shared/transformations/capacity_silver.py` â€” [NEW] Capacity CSVâ†’Silver
- `functions/shared/transformations/maintenance_silver.py` â€” [NEW] Maintenance JSONâ†’Silver
- `functions/shared/transformations/era5_silver.py` â€” [NEW] ERA5 Parquetâ†’Silver
- `tests/test_transformations.py` â€” [NEW] 17 tests

### Change Log

- 2026-02-26: Story completed. 17/17 tests pass.
